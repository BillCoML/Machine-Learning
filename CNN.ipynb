{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "data = pd.read_csv('/Users/tanhoangminhco/Documents/Coding/Python/Machine Learning/datasets/mnist_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self):\n",
    "        self.bpg_input = None\n",
    "        self.convo1 = None\n",
    "        self.convo2 = None\n",
    "        self.pooled = None\n",
    "        self.neurons = 15\n",
    "        self.m = None\n",
    "        self.W1, self.b1, self.W2, self.b2 = None, None, None, None\n",
    "        self.X_train, self.Y_train, self.X_test, self.Y_test = None, None, None, None\n",
    "        self.kernel = None\n",
    "        self.best_kernel = None\n",
    "        self.N_of_kernel = 5\n",
    "        self.Len_of_kernel = 1\n",
    "        self.iter_each_epoch = 3000\n",
    "        self.epoch = 2\n",
    "        self.alpha = 0.1\n",
    "        self.highest_train_accuracy = 0\n",
    "\n",
    "    def prepare_data(self, data, test_size=0.9):\n",
    "        data = np.array(data)\n",
    "        m, n = data.shape\n",
    "        np.random.shuffle(data)\n",
    "        test_size = int(m * test_size)\n",
    "\n",
    "        data_test = data[:test_size].T #785 x test_size\n",
    "        Y_test = data_test[0] #test_size x 1\n",
    "        X_test = data_test[1:n] #784 x test_size\n",
    "        X_test = X_test / 255.\n",
    "\n",
    "        data_train = data[test_size:].T #785 x train_size\n",
    "        Y_train = data_train[0] #train_size x 1\n",
    "        X_train = data_train[1:n]\n",
    "        X_train = X_train / 255.\n",
    "\n",
    "        self.m = m - test_size\n",
    "\n",
    "        return X_train, Y_train, X_test, Y_test\n",
    "    \n",
    "    def input_For_Back_prop(self, X_Train, stride=1):\n",
    "        #This method should be used only once\n",
    "        a, b = X_Train.shape\n",
    "        len_kernel = self.Len_of_kernel\n",
    "        max = int(math.sqrt(a))\n",
    "        res = []\n",
    "        for i in range(b):\n",
    "            data = X_Train[:,i]\n",
    "            data = data.reshape(max,max)\n",
    "            r = 0\n",
    "            while (r+len_kernel <= max):\n",
    "                c = 0\n",
    "                while (c+len_kernel <= max):\n",
    "                    x = data[r:r+len_kernel, c:c+len_kernel]\n",
    "                    res.append(x.reshape(1,-1))\n",
    "                    c += stride\n",
    "                r += stride\n",
    "        return np.array(res).reshape(self.m, -1, len_kernel**2)\n",
    "    \n",
    "    def conv(self, BigData, kernels, stride=1):\n",
    "        a, b = BigData.shape\n",
    "        max = int(math.sqrt(a))\n",
    "        res = []\n",
    "        k = self.Len_of_kernel\n",
    "\n",
    "        for i in range(b):\n",
    "            data = BigData[:,i]\n",
    "            data = data.reshape(max,max)\n",
    "\n",
    "            for kernel in kernels:\n",
    "                r = 0\n",
    "                while (r+k <= max):\n",
    "                    c = 0\n",
    "                    while (c+k <= max):\n",
    "                        x = data[r:r+k, c:c+k] * kernel\n",
    "                        res.append(np.sum(x))\n",
    "                        c += stride\n",
    "                    r += stride\n",
    "        size_of_each = int(len(res)/(b * self.N_of_kernel))\n",
    "        self.convo = np.array(res).reshape(b, self.N_of_kernel, 1, size_of_each)\n",
    "        return self.convo\n",
    "    \n",
    "    def pooling(self, BigData, k=4):\n",
    "        a,b,_,d = BigData.shape\n",
    "        max = int(math.sqrt(d))\n",
    "        res = []\n",
    "        for i in range(a):\n",
    "            for y in range(b):\n",
    "                r = 0\n",
    "                data = BigData[i][y][0]\n",
    "                data = data.reshape(max,max)\n",
    "                while (r+k <= max):\n",
    "                    c = 0\n",
    "                    while (c+k <= max):\n",
    "                        x = data[r:r+k, c:c+k]\n",
    "                        res.append(np.max(x))\n",
    "                        c += k\n",
    "                    r += k\n",
    "        size_of_each = int(len(res)/(a*b))\n",
    "        self.pooled = np.array(res).reshape(a,b,_,size_of_each)\n",
    "        return self.pooled\n",
    "    \n",
    "    def flatten(self, BigData):\n",
    "        m,k,_,size = BigData.shape\n",
    "        return BigData.reshape(m,size*k).T\n",
    "    \n",
    "    def draw(self, current_image):\n",
    "        current_image = current_image[0]\n",
    "        size = int(math.sqrt(len(current_image)))\n",
    "        current_image = current_image.reshape((size,size)) * 255\n",
    "        plt.gray()\n",
    "        plt.imshow(current_image, interpolation='nearest')\n",
    "        plt.show()\n",
    "    \n",
    "    def init_params(self, n):\n",
    "        W1 = np.random.rand(self.neurons, n) - 0.5\n",
    "        b1 = np.random.rand(self.neurons,1) - 0.5\n",
    "        W2 = np.random.rand(10, self.neurons) - 0.5\n",
    "        b2 = np.random.rand(10,1) - 0.5\n",
    "        return W1, b1, W2, b2\n",
    "    \n",
    "    def one_hot(self, Y):\n",
    "        one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "        one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "        one_hot_Y = one_hot_Y.T\n",
    "        return one_hot_Y\n",
    "    \n",
    "    def ReLU(self, Z):\n",
    "        return np.maximum(0,Z)\n",
    "    \n",
    "    def SoftMax(self, A):\n",
    "        A = np.exp(A) / sum(np.exp(A))\n",
    "        return A\n",
    "    \n",
    "    def ReLU_deriv(self, Z):\n",
    "        return Z > 0\n",
    "    \n",
    "    def poolingAndConv_deriv(self, BigData, pooled, k=4):\n",
    "        a,b,_,d = BigData.shape\n",
    "        max = int(math.sqrt(d))\n",
    "        pooled = pooled.reshape(a,b,-1, int(max/k))\n",
    "        for i in range(a):\n",
    "            for y in range(b):\n",
    "                r = 0\n",
    "                p_row = 0\n",
    "                data = BigData[i][y][0]\n",
    "                pool = pooled[i][y]\n",
    "                data = data.reshape(max,max)\n",
    "                while (r+k <= max):\n",
    "                    c = 0\n",
    "                    p_col = 0\n",
    "                    while (c+k <= max):\n",
    "                        x = data[r:r+k, c:c+k]\n",
    "                        maximum = np.max(x)\n",
    "                        data[r:r+k, c:c+k] = (data[r:r+k, c:c+k] == maximum) * pool[p_row][p_col]\n",
    "                        c += k\n",
    "                        p_col += 1\n",
    "                    \n",
    "                    r += k\n",
    "                    p_row += 1\n",
    "        return BigData\n",
    "    \n",
    "    def get_predictions(self, A2):\n",
    "        return np.argmax(A2, 0)\n",
    "\n",
    "    def get_accuracy(self, predictions, Y):\n",
    "        return np.sum(predictions == Y) / Y.size\n",
    "\n",
    "    def forward_prop(self, W1, b1, W2, b2, X):\n",
    "        Z1 = W1.dot(X)\n",
    "        A1 = self.ReLU(Z1 + b1)\n",
    "        Z2 = W2.dot(A1)\n",
    "        A2 = self.SoftMax(Z2 + b2)\n",
    "        return Z1, A1, A2\n",
    "    \n",
    "    def convo_For_Back_prop(self, convo1):\n",
    "        return convo1.reshape(self.m ,self.N_of_kernel,-1)\n",
    "    \n",
    "    def input_For_Back_prop(self, X_train, stride=1):\n",
    "        #This method should be used only once\n",
    "        a, b = X_train.shape\n",
    "        max = int(math.sqrt(a))\n",
    "        res = []\n",
    "        for i in range(b):\n",
    "            data = X_train[:,i]\n",
    "            data = data.reshape(max,max)\n",
    "            r = 0\n",
    "            while (r+self.Len_of_kernel <= max):\n",
    "                c = 0\n",
    "                while (c+self.Len_of_kernel <= max):\n",
    "                    x = data[r:r+self.Len_of_kernel, c:c+self.Len_of_kernel]\n",
    "                    res.append(x.reshape(1,-1))\n",
    "                    c += stride\n",
    "                r += stride\n",
    "        return np.array(res).reshape(self.m, -1, self.Len_of_kernel ** 2)\n",
    "\n",
    "    def back_prop_convo(self,X,one_hot_Y,Z1,A1,A2,W1,W2, convo1, convo2, pooled, convo_back_prop):\n",
    "        m = self.m\n",
    "        dZ2 = A2 - one_hot_Y\n",
    "        dW2 = 1/m * ( dZ2 @ A1.T )\n",
    "        db2 = 1/m * np.sum(dZ2, axis=1).reshape(-1,1)\n",
    "        dZ1 = W2.T @ dZ2 * self.ReLU_deriv(Z1)\n",
    "        dW1 = 1/m  * ( dZ1 @ X.T )\n",
    "        db1 = 1/m * np.sum(dZ1, axis=1).reshape(-1,1)\n",
    "        ####\n",
    "        dKernel = None\n",
    "        if (convo_back_prop):\n",
    "            dX = 1/m * W1.T @ dZ1\n",
    "            dX = dX.reshape(self.m, self.N_of_kernel , 1, -1)#(m,3,)\n",
    "            dConvo2 = self.poolingAndConv_deriv(convo2, pooled)\n",
    "            dConvo1 = dConvo2 * self.ReLU_deriv(convo1) #(m,3,24,24)\n",
    "            #####\n",
    "            convo1_back = self.convo_For_Back_prop(dConvo1)\n",
    "            dKernel = 1/m * (convo1_back @ self.bpg_input).sum(axis = 0)\n",
    "            dKernel = dKernel.reshape(self.N_of_kernel, self.Len_of_kernel, self.Len_of_kernel)\n",
    "\n",
    "        return dW1, db1, dW2, db2, dKernel\n",
    "    \n",
    "    def gradient_descent(self, X, Y, epochs, alpha):\n",
    "        \n",
    "        # n,_ = conv_X_train.shape\n",
    "        n = 36 * self.N_of_kernel ###Fix later\n",
    "        aW1, aW2, aW3, aW4 = self.init_params(n)\n",
    "        one_hot_Y = self.one_hot(Y.reshape(1,-1))\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            convo_back_prop = True\n",
    "            self.convo1 = self.conv(X, self.kernel)\n",
    "            self.convo2 = self.ReLU(self.convo1)\n",
    "            self.pooled = self.pooling(self.convo2)\n",
    "            conv_X_train = self.flatten(self.pooled)\n",
    "\n",
    "            W1, b1, W2, b2 = aW1, aW2, aW3, aW4\n",
    "            for iter in range(self.iter_each_epoch):\n",
    "                Z1, A1, A2 = self.forward_prop(W1, b1, W2, b2, conv_X_train)\n",
    "                dW1, db1, dW2, db2, dKernel = self.back_prop_convo(conv_X_train, one_hot_Y,Z1,A1,A2,W1,W2, self.convo1, self.convo2, self.pooled, convo_back_prop)\n",
    "                if (iter == 0):\n",
    "                    self.kernel = self.kernel - alpha * dKernel\n",
    "                    convo_back_prop = False\n",
    "                ##Update\n",
    "                W1 = W1 - alpha * dW1\n",
    "                b1 = b1 - alpha * db1\n",
    "                W2 = W2 - alpha * dW2\n",
    "                b2 = b2 - alpha * db2\n",
    "                ##Show accuracy\n",
    "                if (iter % 100 == 0):\n",
    "                    predictions = self.get_predictions(A2)\n",
    "                    acc = self.get_accuracy(predictions, Y)\n",
    "                    print(f'Epoch {epoch+1}, iter {iter}__Accuracy: {acc}')\n",
    "                    if (acc > self.highest_train_accuracy):\n",
    "                        self.highest_train_accuracy = acc\n",
    "                        self.W1, self.b1, self.W2, self.b2 = W1, b1, W2, b2\n",
    "                        self.best_kernel = self.kernel \n",
    "\n",
    "    def fit(self, data):\n",
    "        self.X_train, self.Y_train, self.X_test, self.Y_test = self.prepare_data(data)\n",
    "        self.bpg_input = self.input_For_Back_prop(self.X_train)\n",
    "        self.kernel = np.random.randint(-3,3,(self.N_of_kernel,self.Len_of_kernel,self.Len_of_kernel))\n",
    "        self.gradient_descent(self.X_train, self.Y_train, self.epoch, self.alpha)\n",
    "    \n",
    "    def predict_test(self):\n",
    "        data = self.X_test[:,:]\n",
    "        conv_test = self.ReLU(self.conv(data,self.best_kernel))\n",
    "        conv_X_test = self.flatten(self.pooling(conv_test))\n",
    "        _, _, A2 = self.forward_prop(self.W1, self.b1, self.W2, self.b2, conv_X_test)\n",
    "        predictions = self.get_predictions(A2)\n",
    "        acc = self.get_accuracy(predictions, self.Y_test)\n",
    "        print(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (15,180) and (245,1000) not aligned: 180 (dim 1) != 245 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m cnn \u001b[38;5;241m=\u001b[39m CNN()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 258\u001b[0m, in \u001b[0;36mCNN.fit\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbpg_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_For_Back_prop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_train)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m,(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN_of_kernel,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLen_of_kernel,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLen_of_kernel))\n\u001b[0;32m--> 258\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 234\u001b[0m, in \u001b[0;36mCNN.gradient_descent\u001b[0;34m(self, X, Y, epochs, alpha)\u001b[0m\n\u001b[1;32m    232\u001b[0m W1, b1, W2, b2 \u001b[38;5;241m=\u001b[39m aW1, aW2, aW3, aW4\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_each_epoch):\n\u001b[0;32m--> 234\u001b[0m     Z1, A1, A2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_prop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_X_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m     dW1, db1, dW2, db2, dKernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mback_prop_convo(conv_X_train, one_hot_Y,Z1,A1,A2,W1,W2, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvo1, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvo2, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooled, convo_back_prop)\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28miter\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "Cell \u001b[0;32mIn[2], line 169\u001b[0m, in \u001b[0;36mCNN.forward_prop\u001b[0;34m(self, W1, b1, W2, b2, X)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_prop\u001b[39m(\u001b[38;5;28mself\u001b[39m, W1, b1, W2, b2, X):\n\u001b[0;32m--> 169\u001b[0m     Z1 \u001b[38;5;241m=\u001b[39m \u001b[43mW1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     A1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mReLU(Z1 \u001b[38;5;241m+\u001b[39m b1)\n\u001b[1;32m    171\u001b[0m     Z2 \u001b[38;5;241m=\u001b[39m W2\u001b[38;5;241m.\u001b[39mdot(A1)\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (15,180) and (245,1000) not aligned: 180 (dim 1) != 245 (dim 0)"
     ]
    }
   ],
   "source": [
    "cnn = CNN()\n",
    "cnn.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3980612244897959\n"
     ]
    }
   ],
   "source": [
    "cnn.predict_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
